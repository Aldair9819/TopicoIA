{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Directorio que contiene las imágenes de las caras\n",
    "base_dir = 'C:\\\\Users\\\\Waldosir\\\\Documents\\\\2doCodigo\\\\TopicoIA\\\\Marzo\\\\BD\\\\corpus_images_chat\\\\Train\\\\'\n",
    "\n",
    "# Listas para almacenar las caras y las etiquetas correspondientes\n",
    "images = []\n",
    "labels = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\208.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\225.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\226.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\228.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\230.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\234.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\239.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\240.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\242.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\243.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\250.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\251.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\255.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\bored\\256.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\engaged\\identificador_1558_2016-11-28_14-55-22.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\engaged\\identificador_5867_2016-12-05_15-02-28.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\engaged\\identificador_993_2016-11-09_14-05-22.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\excited\\identificador_5772_2016-12-05_14-31-53.jpeg: list index out of range\n",
      "Error al procesar la imagen C:\\Users\\Waldosir\\Documents\\2doCodigo\\TopicoIA\\Marzo\\BD\\corpus_images_chat\\Train\\excited\\identificador_994_2016-11-09_14-05-26.jpeg: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre cada directorio en el directorio base\n",
    "for label in os.listdir(base_dir):\n",
    "    label_dir = os.path.join(base_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        # Obtener las imágenes en el directorio y sus etiquetas\n",
    "        for filename in os.listdir(label_dir):\n",
    "            try:\n",
    "                image_path = os.path.join(label_dir, filename)\n",
    "                image = face_recognition.load_image_file(image_path)\n",
    "                # Redimensionar la imagen a 150x150\n",
    "                image = np.array(Image.fromarray(image).resize((150, 150)))\n",
    "                face_encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "                images.append(face_encodings[0])  # Tomamos solo la codificación facial del primer rostro encontrado\n",
    "                labels.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error al procesar la imagen {image_path}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[{'chin': [(58, 85), (58, 91), (58, 97), (59, 103), (61, 108), (63, 114), (67, 118), (72, 122), (78, 123), (83, 121), (87, 117), (90, 112), (92, 107), (93, 101), (93, 95), (94, 90), (94, 84)], 'left_eyebrow': [(63, 80), (65, 77), (68, 76), (71, 77), (74, 78)], 'right_eyebrow': [(81, 78), (84, 76), (87, 76), (90, 77), (91, 79)], 'nose_bridge': [(78, 83), (78, 86), (78, 90), (78, 94)], 'nose_tip': [(75, 98), (76, 98), (78, 99), (80, 98), (81, 97)], 'left_eye': [(66, 85), (68, 84), (70, 84), (73, 85), (71, 86), (68, 86)], 'right_eye': [(82, 85), (84, 84), (86, 84), (88, 85), (86, 86), (84, 86)], 'top_lip': [(72, 107), (74, 105), (76, 104), (78, 104), (80, 104), (82, 105), (83, 107), (82, 107), (80, 106), (78, 106), (76, 106), (73, 107)], 'bottom_lip': [(83, 107), (82, 109), (80, 110), (78, 111), (76, 111), (74, 110), (72, 107), (73, 107), (76, 107), (78, 107), (80, 107), (82, 107)]}\n {'chin': [(59, 84), (59, 89), (59, 95), (60, 101), (61, 106), (64, 112), (68, 116), (73, 119), (78, 121), (84, 119), (88, 115), (91, 111), (93, 105), (94, 100), (94, 94), (95, 89), (95, 84)], 'left_eyebrow': [(64, 79), (66, 76), (69, 75), (73, 75), (76, 77)], 'right_eyebrow': [(83, 77), (86, 76), (89, 75), (92, 76), (93, 79)], 'nose_bridge': [(80, 82), (80, 86), (80, 89), (80, 93)], 'nose_tip': [(76, 97), (78, 97), (80, 98), (81, 97), (83, 97)], 'left_eye': [(68, 84), (70, 83), (72, 83), (74, 84), (72, 85), (70, 85)], 'right_eye': [(84, 84), (86, 83), (88, 83), (90, 84), (88, 85), (86, 85)], 'top_lip': [(73, 106), (76, 104), (78, 103), (80, 103), (81, 103), (83, 104), (85, 106), (84, 106), (81, 105), (79, 105), (78, 105), (74, 106)], 'bottom_lip': [(85, 106), (83, 108), (81, 109), (79, 109), (77, 109), (75, 108), (73, 106), (74, 106), (78, 106), (79, 106), (81, 106), (84, 106)]}\n {'chin': [(59, 84), (59, 90), (59, 96), (60, 101), (61, 107), (64, 112), (68, 116), (72, 120), (78, 121), (83, 120), (87, 116), (90, 111), (92, 106), (93, 101), (94, 95), (95, 90), (94, 85)], 'left_eyebrow': [(64, 79), (66, 76), (69, 75), (73, 76), (76, 77)], 'right_eyebrow': [(83, 77), (86, 76), (89, 76), (92, 77), (93, 79)], 'nose_bridge': [(80, 82), (80, 85), (80, 89), (80, 93)], 'nose_tip': [(76, 97), (78, 97), (79, 98), (81, 97), (82, 97)], 'left_eye': [(68, 84), (70, 82), (72, 82), (74, 84), (72, 84), (70, 85)], 'right_eye': [(83, 84), (86, 82), (88, 83), (89, 84), (88, 85), (86, 85)], 'top_lip': [(72, 106), (75, 104), (78, 103), (79, 104), (81, 103), (83, 104), (85, 106), (83, 106), (81, 105), (79, 105), (77, 105), (74, 106)], 'bottom_lip': [(85, 106), (83, 108), (81, 109), (79, 109), (77, 109), (75, 108), (72, 106), (74, 106), (77, 106), (79, 106), (81, 106), (83, 106)]}\n ...\n {'chin': [(66, 77), (66, 83), (67, 89), (68, 96), (71, 101), (75, 106), (79, 110), (84, 112), (90, 112), (94, 110), (97, 106), (99, 102), (101, 96), (103, 91), (104, 86), (103, 81), (103, 77)], 'left_eyebrow': [(74, 68), (77, 66), (81, 65), (84, 65), (87, 67)], 'right_eyebrow': [(92, 68), (94, 66), (97, 66), (99, 67), (100, 69)], 'nose_bridge': [(90, 73), (90, 76), (91, 79), (91, 83)], 'nose_tip': [(86, 87), (88, 87), (90, 88), (92, 87), (94, 86)], 'left_eye': [(77, 74), (80, 73), (82, 73), (84, 74), (82, 75), (80, 75)], 'right_eye': [(93, 75), (95, 73), (97, 74), (99, 75), (97, 76), (95, 76)], 'top_lip': [(80, 94), (84, 92), (88, 91), (90, 91), (92, 91), (95, 92), (96, 94), (95, 94), (92, 93), (90, 93), (88, 93), (81, 95)], 'bottom_lip': [(96, 94), (94, 98), (92, 100), (90, 100), (87, 100), (84, 98), (80, 94), (81, 95), (88, 97), (90, 97), (92, 97), (95, 94)]}\n {'chin': [(64, 78), (65, 84), (65, 90), (66, 97), (69, 102), (72, 107), (76, 111), (82, 114), (87, 114), (91, 113), (95, 109), (97, 104), (99, 99), (100, 94), (101, 89), (101, 84), (100, 79)], 'left_eyebrow': [(71, 71), (74, 69), (78, 67), (82, 68), (85, 70)], 'right_eyebrow': [(90, 70), (93, 69), (96, 68), (98, 69), (100, 72)], 'nose_bridge': [(87, 75), (88, 79), (88, 82), (88, 86)], 'nose_tip': [(83, 89), (85, 90), (88, 91), (90, 90), (91, 89)], 'left_eye': [(75, 76), (77, 75), (80, 75), (82, 77), (80, 77), (77, 77)], 'right_eye': [(91, 77), (93, 76), (95, 76), (97, 77), (95, 78), (93, 77)], 'top_lip': [(78, 97), (81, 95), (85, 94), (87, 94), (89, 94), (92, 95), (94, 96), (93, 97), (89, 95), (87, 96), (85, 95), (79, 97)], 'bottom_lip': [(94, 96), (92, 100), (89, 102), (87, 103), (85, 102), (81, 101), (78, 97), (79, 97), (85, 100), (87, 100), (89, 100), (93, 97)]}\n {'chin': [(50, 77), (50, 81), (50, 86), (51, 90), (52, 95), (55, 99), (58, 103), (62, 105), (67, 105), (72, 104), (77, 101), (80, 97), (83, 93), (84, 87), (85, 82), (85, 77), (85, 71)], 'left_eyebrow': [(51, 69), (52, 66), (54, 64), (57, 63), (60, 64)], 'right_eyebrow': [(65, 62), (68, 60), (72, 59), (75, 60), (78, 62)], 'nose_bridge': [(63, 68), (63, 71), (63, 73), (63, 76)], 'nose_tip': [(60, 81), (62, 82), (64, 82), (66, 81), (68, 80)], 'left_eye': [(54, 73), (55, 71), (57, 70), (60, 71), (58, 72), (56, 72)], 'right_eye': [(68, 69), (70, 67), (72, 66), (74, 67), (73, 68), (71, 68)], 'top_lip': [(58, 90), (59, 87), (62, 85), (65, 85), (67, 84), (71, 85), (75, 86), (74, 86), (67, 86), (65, 87), (63, 87), (58, 90)], 'bottom_lip': [(75, 86), (72, 91), (68, 93), (66, 94), (63, 94), (60, 93), (58, 90), (58, 90), (63, 92), (65, 92), (68, 91), (74, 86)]}].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Crear y entrenar el clasificador KNN\u001b[39;00m\n\u001b[0;32m      2\u001b[0m knn_classifier \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Puedes ajustar el número de vecinos según sea necesario\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mknn_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Directorio que contiene las imágenes de prueba\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mWaldosir\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2doCodigo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTopicoIA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMarzo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mBD\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcorpus_images_chat\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:238\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_base.py:476\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 476\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Waldosir\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1029\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m             )\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[{'chin': [(58, 85), (58, 91), (58, 97), (59, 103), (61, 108), (63, 114), (67, 118), (72, 122), (78, 123), (83, 121), (87, 117), (90, 112), (92, 107), (93, 101), (93, 95), (94, 90), (94, 84)], 'left_eyebrow': [(63, 80), (65, 77), (68, 76), (71, 77), (74, 78)], 'right_eyebrow': [(81, 78), (84, 76), (87, 76), (90, 77), (91, 79)], 'nose_bridge': [(78, 83), (78, 86), (78, 90), (78, 94)], 'nose_tip': [(75, 98), (76, 98), (78, 99), (80, 98), (81, 97)], 'left_eye': [(66, 85), (68, 84), (70, 84), (73, 85), (71, 86), (68, 86)], 'right_eye': [(82, 85), (84, 84), (86, 84), (88, 85), (86, 86), (84, 86)], 'top_lip': [(72, 107), (74, 105), (76, 104), (78, 104), (80, 104), (82, 105), (83, 107), (82, 107), (80, 106), (78, 106), (76, 106), (73, 107)], 'bottom_lip': [(83, 107), (82, 109), (80, 110), (78, 111), (76, 111), (74, 110), (72, 107), (73, 107), (76, 107), (78, 107), (80, 107), (82, 107)]}\n {'chin': [(59, 84), (59, 89), (59, 95), (60, 101), (61, 106), (64, 112), (68, 116), (73, 119), (78, 121), (84, 119), (88, 115), (91, 111), (93, 105), (94, 100), (94, 94), (95, 89), (95, 84)], 'left_eyebrow': [(64, 79), (66, 76), (69, 75), (73, 75), (76, 77)], 'right_eyebrow': [(83, 77), (86, 76), (89, 75), (92, 76), (93, 79)], 'nose_bridge': [(80, 82), (80, 86), (80, 89), (80, 93)], 'nose_tip': [(76, 97), (78, 97), (80, 98), (81, 97), (83, 97)], 'left_eye': [(68, 84), (70, 83), (72, 83), (74, 84), (72, 85), (70, 85)], 'right_eye': [(84, 84), (86, 83), (88, 83), (90, 84), (88, 85), (86, 85)], 'top_lip': [(73, 106), (76, 104), (78, 103), (80, 103), (81, 103), (83, 104), (85, 106), (84, 106), (81, 105), (79, 105), (78, 105), (74, 106)], 'bottom_lip': [(85, 106), (83, 108), (81, 109), (79, 109), (77, 109), (75, 108), (73, 106), (74, 106), (78, 106), (79, 106), (81, 106), (84, 106)]}\n {'chin': [(59, 84), (59, 90), (59, 96), (60, 101), (61, 107), (64, 112), (68, 116), (72, 120), (78, 121), (83, 120), (87, 116), (90, 111), (92, 106), (93, 101), (94, 95), (95, 90), (94, 85)], 'left_eyebrow': [(64, 79), (66, 76), (69, 75), (73, 76), (76, 77)], 'right_eyebrow': [(83, 77), (86, 76), (89, 76), (92, 77), (93, 79)], 'nose_bridge': [(80, 82), (80, 85), (80, 89), (80, 93)], 'nose_tip': [(76, 97), (78, 97), (79, 98), (81, 97), (82, 97)], 'left_eye': [(68, 84), (70, 82), (72, 82), (74, 84), (72, 84), (70, 85)], 'right_eye': [(83, 84), (86, 82), (88, 83), (89, 84), (88, 85), (86, 85)], 'top_lip': [(72, 106), (75, 104), (78, 103), (79, 104), (81, 103), (83, 104), (85, 106), (83, 106), (81, 105), (79, 105), (77, 105), (74, 106)], 'bottom_lip': [(85, 106), (83, 108), (81, 109), (79, 109), (77, 109), (75, 108), (72, 106), (74, 106), (77, 106), (79, 106), (81, 106), (83, 106)]}\n ...\n {'chin': [(66, 77), (66, 83), (67, 89), (68, 96), (71, 101), (75, 106), (79, 110), (84, 112), (90, 112), (94, 110), (97, 106), (99, 102), (101, 96), (103, 91), (104, 86), (103, 81), (103, 77)], 'left_eyebrow': [(74, 68), (77, 66), (81, 65), (84, 65), (87, 67)], 'right_eyebrow': [(92, 68), (94, 66), (97, 66), (99, 67), (100, 69)], 'nose_bridge': [(90, 73), (90, 76), (91, 79), (91, 83)], 'nose_tip': [(86, 87), (88, 87), (90, 88), (92, 87), (94, 86)], 'left_eye': [(77, 74), (80, 73), (82, 73), (84, 74), (82, 75), (80, 75)], 'right_eye': [(93, 75), (95, 73), (97, 74), (99, 75), (97, 76), (95, 76)], 'top_lip': [(80, 94), (84, 92), (88, 91), (90, 91), (92, 91), (95, 92), (96, 94), (95, 94), (92, 93), (90, 93), (88, 93), (81, 95)], 'bottom_lip': [(96, 94), (94, 98), (92, 100), (90, 100), (87, 100), (84, 98), (80, 94), (81, 95), (88, 97), (90, 97), (92, 97), (95, 94)]}\n {'chin': [(64, 78), (65, 84), (65, 90), (66, 97), (69, 102), (72, 107), (76, 111), (82, 114), (87, 114), (91, 113), (95, 109), (97, 104), (99, 99), (100, 94), (101, 89), (101, 84), (100, 79)], 'left_eyebrow': [(71, 71), (74, 69), (78, 67), (82, 68), (85, 70)], 'right_eyebrow': [(90, 70), (93, 69), (96, 68), (98, 69), (100, 72)], 'nose_bridge': [(87, 75), (88, 79), (88, 82), (88, 86)], 'nose_tip': [(83, 89), (85, 90), (88, 91), (90, 90), (91, 89)], 'left_eye': [(75, 76), (77, 75), (80, 75), (82, 77), (80, 77), (77, 77)], 'right_eye': [(91, 77), (93, 76), (95, 76), (97, 77), (95, 78), (93, 77)], 'top_lip': [(78, 97), (81, 95), (85, 94), (87, 94), (89, 94), (92, 95), (94, 96), (93, 97), (89, 95), (87, 96), (85, 95), (79, 97)], 'bottom_lip': [(94, 96), (92, 100), (89, 102), (87, 103), (85, 102), (81, 101), (78, 97), (79, 97), (85, 100), (87, 100), (89, 100), (93, 97)]}\n {'chin': [(50, 77), (50, 81), (50, 86), (51, 90), (52, 95), (55, 99), (58, 103), (62, 105), (67, 105), (72, 104), (77, 101), (80, 97), (83, 93), (84, 87), (85, 82), (85, 77), (85, 71)], 'left_eyebrow': [(51, 69), (52, 66), (54, 64), (57, 63), (60, 64)], 'right_eyebrow': [(65, 62), (68, 60), (72, 59), (75, 60), (78, 62)], 'nose_bridge': [(63, 68), (63, 71), (63, 73), (63, 76)], 'nose_tip': [(60, 81), (62, 82), (64, 82), (66, 81), (68, 80)], 'left_eye': [(54, 73), (55, 71), (57, 70), (60, 71), (58, 72), (56, 72)], 'right_eye': [(68, 69), (70, 67), (72, 66), (74, 67), (73, 68), (71, 68)], 'top_lip': [(58, 90), (59, 87), (62, 85), (65, 85), (67, 84), (71, 85), (75, 86), (74, 86), (67, 86), (65, 87), (63, 87), (58, 90)], 'bottom_lip': [(75, 86), (72, 91), (68, 93), (66, 94), (63, 94), (60, 93), (58, 90), (58, 90), (63, 92), (65, 92), (68, 91), (74, 86)]}].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el clasificador KNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)  # Puedes ajustar el número de vecinos según sea necesario\n",
    "knn_classifier.fit(images, labels)\n",
    "\n",
    "# Directorio que contiene las imágenes de prueba\n",
    "test_dir = 'C:\\\\Users\\\\Waldosir\\\\Documents\\\\2doCodigo\\\\TopicoIA\\\\Marzo\\\\BD\\\\corpus_images_chat\\\\Test'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir las etiquetas de las imágenes de prueba\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((150, 150))  # Redimensionar la imagen\n",
    "        image = face_recognition.array(image)\n",
    "        landmarks = face_recognition.face_landmarks(image)\n",
    "        if landmarks:\n",
    "            predicted_label = knn_classifier.predict([landmarks[0]])\n",
    "            print(f'La imagen {filename} parece ser: {predicted_label[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
